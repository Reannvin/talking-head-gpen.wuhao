import subprocess
import os
import argparse
from yoloface.face_detector import YoloDetector
from PIL import Image
import cv2
from tqdm import tqdm
from Evaluation.face_detection import FaceAlignment, LandmarksType
import torch
from utils.lm_detection import get_landmark_and_bbox
import logging
from mmpose.apis import init_model

LOG_FORMAT = "%(asctime)s - %(levelname)s - %(message)s"
logging.basicConfig(filename='detect.log', level=logging.INFO, format=LOG_FORMAT)

def extract_audio(video_file, audio_file):
    cmd = f'ffmpeg -loglevel panic -y -i {video_file} -strict -2 {audio_file}'
    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = process.communicate()
    if process.returncode != 0:
        error_msg = f"分离音视频失败: {stderr.decode('utf-8')}"
        logging.error(error_msg)
    else:
        logging.info(f"音频成功提取到: {audio_file}")

def read_img_from_video(video_path, method):
    video_stream = cv2.VideoCapture(video_path)
    frames = []
    while True:
        still_reading, frame = video_stream.read()
        if not still_reading:
            video_stream.release()
            break
        if method == 'yolo':
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames.append(frame)
    return frames

def Yolo(video_path, gpu_id, frames, output_dir):
    model = YoloDetector(target_size=None, device=f"cuda:{gpu_id}", min_face=90)
    crop_images = []
    for i, frame in enumerate(frames):
        bboxes, points = model.predict(frame)
        if bboxes:
            x1, y1, x2, y2 = bboxes[0][0]
            x1, x2 = max(0, x1), min(frame.shape[1], x2)
            y1, y2 = max(0, y1), min(frame.shape[0], y2)
            crop_img = frame[y1:y2, x1:x2]
            crop_images.append(crop_img)
            img = Image.fromarray(crop_img)
            img.save(f"{output_dir}/{i}.png")
        else:
            logging.warning(f"未检测到脸部框架在帧 {i}")
    logging.info("YOLO 检测完成")
def detect_and_crop(video_files, person_ids, method,output_root,gpu_id):
    for video_file, person_id in tqdm(zip(video_files, person_ids)):
        frames = read_img_from_video(video_file, method)
        save_dir = os.path.join(output_root, person_id)
        os.makedirs(save_dir, exist_ok=True)

        if args.method == 'yolo':
            Yolo(video_file, gpu_id, frames, save_dir)
        else:
            device = torch.device(f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu")
            config_file = './models/dwpose/rtmpose-l_8xb32-270e_coco-ubody-wholebody-384x288.py'
            checkpoint_file = './models/dwpose/dw-ll_ucoco_384.pth'
            model = init_model(config_file, checkpoint_file, device=device)     
            fa = FaceAlignment(LandmarksType._2D, flip_input=False, device=device)
            coord_placeholder = (0.0, 0.0, 0.0, 0.0)
            get_landmark_and_bbox(frames, fa, model, coord_placeholder, save_dir, upperbondrange=0, btm_scalw=0.1)
            logging.info("MMPose 检测完成")
        audio_file = os.path.join(save_dir, "audio.wav")
        extract_audio(video_file, audio_file)
    logging.info("********* 处理完成 ************")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    parser = argparse.ArgumentParser(description='data preprocessing')
    parser.add_argument("--output_root", default='/data/fanshen/workspace/preprocessed_avspeech', help="your output root path", type=str)
    parser.add_argument("--gpu_id", default=0, help="gpu use for face detection", type=int)
    parser.add_argument("--method", default='mmpose', choices=['mmpose', 'yolo'], help="face detection method", type=str)
    parser.add_argument("--filelist", help="file list generated by scene detect", type=str)
    args = parser.parse_args()
    person_ids = []
    video_files = []
    with open(args.filelist, 'r') as f:
        lines = f.readlines()
        for line in lines:
            video_files.append(line.split(' ')[0])
            person_ids.append(line.split(' ')[-1].strip())
    
    logging.info("********* 开始人脸剪裁 ************")
    detect_and_crop(video_files, person_ids, args.method,args.output_root,args.gpu_id)
    